{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGczC83CBIZc/3jXhTGlju"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1z21rlSJfFwI"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters\n","input_size = 784 # 28x28\n","hidden_size = 500\n","num_classes = 10\n","num_epochs = 2\n","batch_size = 100\n","learning_rate = 0.001\n","\n","# MNIST dataset\n","train_dataset = torchvision.datasets.MNIST(root='./data',\n","                                           train=True,\n","                                           transform=transforms.ToTensor(),\n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.MNIST(root='./data',\n","                                          train=False,\n","                                          transform=transforms.ToTensor())\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","examples = iter(test_loader)\n","example_data, example_targets = next(examples)\n","\n","for i in range(6):\n","    plt.subplot(2,3,i+1)\n","    plt.imshow(example_data[i][0], cmap='gray')\n","plt.show()\n","\n","# Fully connected neural network with one hidden layer\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.input_size = input_size\n","        self.l1 = nn.Linear(input_size, hidden_size)\n","        self.relu = nn.ReLU()\n","        self.l2 = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        out = self.l1(x)\n","        out = self.relu(out)\n","        out = self.l2(out)\n","        # no activation and no softmax at the end\n","        return out\n","\n","model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","n_total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # origin shape: [100, 1, 28, 28]\n","        # resized: [100, 784]\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n","\n","# Test the model\n","# In test phase, we don't need to compute gradients (for memory efficiency)\n","with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","    for images, labels in test_loader:\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        # max returns (value ,index)\n","        _, predicted = torch.max(outputs.data, 1)\n","        n_samples += labels.size(0)\n","        n_correct += (predicted == labels).sum().item()\n","\n","    acc = 100.0 * n_correct / n_samples\n","    print(f'Accuracy of the network on the 10000 test images: {acc} %')"]}]}