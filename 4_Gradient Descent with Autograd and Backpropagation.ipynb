{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP+SMpkmFzcYbZJIoMLxzsP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Numpy used"],"metadata":{"id":"WSvFdlubV8wH"}},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eP6ChZewP8ql","executionInfo":{"status":"ok","timestamp":1704302570634,"user_tz":-330,"elapsed":581,"user":{"displayName":"Rohit Diwane","userId":"16455405406727471674"}},"outputId":"1da543bd-75b7-4370-9088-35570ddf26ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["prediction before training : f(5) = 0.000\n","epoch 1: w = 1.200000, loss = 30.00000000\n","epoch 3: w = 1.872000, loss = 0.76800019\n","epoch 5: w = 1.979520, loss = 0.01966083\n","epoch 7: w = 1.996723, loss = 0.00050331\n","epoch 9: w = 1.999476, loss = 0.00001288\n","epoch 11: w = 1.999916, loss = 0.00000033\n","epoch 13: w = 1.999987, loss = 0.00000001\n","epoch 15: w = 1.999998, loss = 0.00000000\n","epoch 17: w = 2.000000, loss = 0.00000000\n","epoch 19: w = 2.000000, loss = 0.00000000\n","prediction after training : f(5) = 10.000\n"]}],"source":["import numpy as np\n","\n","# f = w * x         # linera eq.\n","\n","# f = 2 * x\n","x = np.array([1,2,3,4], dtype=np.float32)\n","y = np.array([2,4,6,8], dtype=np.float32)\n","\n","w = 0.0\n","\n","# model prediction\n","def forward(x):\n","  return w * x\n","\n","# loss  = MSE\n","def loss(y, y_pred):\n","  return ((y_pred-y)**2).mean()\n","\n","# gradient\n","\n","def gradient(x, y, y_pred):\n","  return np.dot(2*x, y_pred-y).mean()\n","\n","print(f'prediction before training : f(5) = {forward(5):.3f}')\n","\n","# training\n","\n","learning_rate = 0.01\n","n_iters = 20\n","\n","for epoch in range(n_iters):\n","  # prediction = forward pass\n","  y_pred = forward(x)\n","\n","  # loss\n","  l = loss(y, y_pred)\n","\n","  # gradient\n","  dw = gradient(x, y, y_pred)\n","\n","  # update weights\n","  w -= learning_rate * dw\n","\n","  if epoch % 2 == 0:  # printing steps\n","    print(f'epoch {epoch+1}: w = {w:3f}, loss = {l:.8f}')\n","\n","print(f'prediction after training : f(5) = {forward(5):.3f}')"]},{"cell_type":"markdown","source":["# Tensor used"],"metadata":{"id":"slQAytyRWCfS"}},{"cell_type":"code","source":["import torch\n","\n","# f = w * x         # linera eq.\n","\n","# f = 2 * x\n","x = torch.tensor([1,2,3,4], dtype=torch.float32)\n","y = torch.tensor([2,4,6,8], dtype=torch.float32)\n","\n","w = torch.tensor(0.0, dtype= torch.float32, requires_grad=True)\n","\n","\n","# model prediction\n","def forward(x):\n","  return w * x\n","\n","# loss = MSE\n","def loss(y, y_pred):\n","  return ((y_pred-y)**2).mean()\n","\n","print(f'prediction before training : f(5) = {forward(5):.3f}')\n","\n","# training\n","\n","learning_rate = 0.01\n","n_iters = 100\n","\n","for epoch in range(n_iters):\n","  # prediction = forward pass\n","  y_pred = forward(x)\n","\n","  # loss\n","  l = loss(y, y_pred)\n","\n","  # gradients = backward pass\n","  l.backward() # dl/dw\n","\n","  # update weights\n","  with torch.no_grad():\n","    w -= learning_rate * w.grad\n","\n","  # zero gradients\n","  w.grad.zero_()\n","\n","  if epoch % 10 == 0:  # printing every 10 steps\n","    print(f'epoch {epoch+1}: w = {w:3f}, loss = {l:.8f}')\n","\n","print(f'prediction after training : f(5) = {forward(5):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_amlRG7FP_BF","executionInfo":{"status":"ok","timestamp":1704303058379,"user_tz":-330,"elapsed":6,"user":{"displayName":"Rohit Diwane","userId":"16455405406727471674"}},"outputId":"5991fcb2-a63a-432d-f923-e1701d663f76"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["prediction before training : f(5) = 0.000\n","epoch 1: w = 0.300000, loss = 30.00000000\n","epoch 11: w = 1.665314, loss = 1.16278565\n","epoch 21: w = 1.934109, loss = 0.04506890\n","epoch 31: w = 1.987028, loss = 0.00174685\n","epoch 41: w = 1.997446, loss = 0.00006770\n","epoch 51: w = 1.999497, loss = 0.00000262\n","epoch 61: w = 1.999901, loss = 0.00000010\n","epoch 71: w = 1.999980, loss = 0.00000000\n","epoch 81: w = 1.999996, loss = 0.00000000\n","epoch 91: w = 1.999999, loss = 0.00000000\n","prediction after training : f(5) = 10.000\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5WGtutdiP_pN"},"execution_count":null,"outputs":[]}]}